from io import BytesIO
from flask import Flask, jsonify
import os
from config import Config
# import tweepy
import snscrape.modules.instagram as snsinsta
from dotenv import load_dotenv
from flask import request,jsonify
import snscrape.modules.twitter as snstwitter
from snscrape.modules.twitter import TwitterSearchScraper, TwitterSearchScraperMode
import requests
import csv
from goose3 import Goose
from wordcloud import WordCloud, STOPWORDS
import plotly.graph_objs as go
import json
import plotly
import matplotlib.pyplot as plt
from PIL import Image
import numpy as np
import base64
import pandas as pd
# from flask import send_file
from flask import send_file
import datetime
import plotly.express as px
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer


app = Flask(__name__)

twitterData = None
queryString = None

# print(type(twitterData))

load_dotenv()

print(os.getenv("HUGGINGFACE_API"))


@app.route('/')
def hello_geek():
    return '<h1>Hello from Flask & Docker</h2>'

@app.route('/twitter')
def twitter():
    query = request.args['query']
    retweet = 0
    likecount = 0
    hashtags = set([])
    i=0
    global twitterData
    global queryString
    print("Url: Twitter, data: ", twitterData)
    print("Url: Twitter, query: ", queryString)
    # if twitterData is None:
    #     twitterData = snstwitter.TwitterSearchScraper(query).get_items()
    #     queryString = query
    # else:
    #     if queryString != query:
    #         twitterData = snstwitter.TwitterSearchScraper(query).get_items()
    #         queryString = query
    #     else:
    #         print(vars(twitterData)) 
    #         print("not scraping again")
    # twitter_scraper = TwitterSearchScraper(query)
    # twitterData = list(twitter_scraper.get_items(TwitterSearchScraperMode.TOP))
    twitterData = snstwitter.TwitterSearchScraper(query).get_items()
        
    for tweet in twitterData: 
        print("looping through tweets")
        print(vars(tweet)) 
        likecount += tweet.likeCount
        retweet += tweet.retweetCount + tweet.quoteCount
        if(tweet.hashtags != None):
            for h in tweet.hashtags:
                hashtags.add(h)
        
        i+= 1
        
        if(i==200):
            break
        
    tweets = {"likecount":likecount,"retweet":retweet,"hashtags":list(hashtags),"count":i}
    print(tweets)
    return jsonify({'result':tweets})


@app.route('/xyz')
def xyz():
    query = request.args['query']
    tweets = []
    for tweet in snstwitter.TwitterProfileScraper(query).get_items():
        tweets.append(tweet.date)
    return tweets



API_URL = "https://api-inference.huggingface.co/models/facebook/bart-large-cnn"
headers = {"Authorization": "Bearer " +  os.getenv('HUGGINGFACE_API') }
API_URL_PROP = "https://api-inference.huggingface.co/models/valurank/distilroberta-propaganda-2class"
API_URL_HATE = "https://api-inference.huggingface.co/models/IMSyPP/hate_speech_en"



def query(payload):
	response = requests.post(API_URL, headers=headers, json=payload)
	return response.json()

def queryprop(payload):
	response = requests.post(API_URL_PROP, headers=headers, json=payload)
	return response.json()

def query_hate(payload):
	response = requests.post(API_URL_HATE, headers=headers, json=payload)
	return response.json()



@app.route('/sentiment')
def sentiment():
    query = request.args['query']
    retweet = 0
    likecount = 0
    hashtags = []
    senti=[]
    i=0
    positive=0
    negative=0
    neutral=0
    global twitterData
    global queryString
    print("Url: Sentiment, data: ", twitterData)
    # if twitterData is None:
    #     twitterData = snstwitter.TwitterSearchScraper(query).get_items()
    #     queryString = query
    # else:
    #     if queryString != query:
    #         twitterData = snstwitter.TwitterSearchScraper(query).get_items()
    #         queryString = query
    twitterData = snstwitter.TwitterSearchScraper(query).get_items()
        
    for tweet in twitterData: 
        if tweet.lang=="en":
            i+=1
            if(i==200):
                break
            sentence= tweet.rawContent
            print(sentence)
            sid_obj = SentimentIntensityAnalyzer()
            sentiment_dict = sid_obj.polarity_scores([sentence])
            print(sentiment_dict['neg']*100, "% Negative")
            print(sentiment_dict['pos']*100, "% Positive")
            print("Review Overall Analysis", end = " ") 
            if sentiment_dict['compound'] >= 0.05 :
                positive+=1
            elif sentiment_dict['compound'] <= -0.05 :
                negative+=1
            else :
                neutral+=1
    senti={"positive":positive, "negative":negative, "neutral":neutral}
    labels = list(senti.keys())
    values = list(senti.values())
        
    return {"labels":labels, "values":values}
            
@app.route('/sentiment_article')
def sentiment_article():
    senti=[]
    url = 'https://blogs.jayeshvp24.dev/dive-into-web-design'
    goose = Goose()
    articles = goose.extract(url)
    sentence1 = articles.cleaned_text
    sid_obj = SentimentIntensityAnalyzer()
    sentiment_dict = sid_obj.polarity_scores([sentence1])
    print(sentiment_dict['neg']*100, "% Negative")
    print(sentiment_dict['pos']*100, "% Positive")
    print("Review Overall Analysis", end = " ") 
    if sentiment_dict['compound'] >= 0.05 :
        senti.append("Positive")
    elif sentiment_dict['compound'] <= -0.05 :
        senti.append("Negative")
    else :
        senti.append("Neutral")
    return jsonify({"result":senti})



@app.route('/summary')
def summary():
    try:

        url = request.args['url']
        goose = Goose()
        articles = goose.extract(url)
        output = query({
        "inputs":  articles.cleaned_text
        })
        print(output)
    except:
        return "Please put the relevant text article"

    return jsonify({"result": output[0]['summary_text']})

@app.route('/cloud2')
def plotly_wordcloud2():
    url = request.args['url']
    goose = Goose()
    articles = goose.extract(url)
    text = articles.cleaned_text
    wordcloud = WordCloud(width=1280, height=853, margin=0,
                      colormap='Blues').generate(text)
    wordcloud.to_file("./wordcloud.png")
    plt.imshow(wordcloud, interpolation='bilinear')
    plt.axis('off')
    plt.margins(x=0, y=0)
    
    # plt.show()
    # img = BytesIO()

    # plt.savefig("./wordcloud.png", format='png')
    # plt.imsave("./wordcloud.png", format='png')
    # img.seek(0)
    # # nimg = Image.frombytes("RGBA", (128, 128), img, 'raw')
    # nimg = Image.frombuffer(img)
    # nimg.save("./wordcloud.png")
    # plot_url = base64.b64encode(img.getvalue()).decode('utf8')
    return send_file("./wordcloud.png", mimetype='image/png')
    # return render_template('plot.html', plot_url=plot_url)

# @app.route('/cloud')
# def plotly_wordcloud():
#     url = 'https://blogs.jayeshvp24.dev/dive-into-web-design'
#     goose = Goose()
#     articles = goose.extract(url)
#     text = query({
# 	"inputs":  articles.cleaned_text
#     })
#     wc = WordCloud(stopwords = set(STOPWORDS),
#                    max_words = 200,
#                    max_font_size = 100)
#     wc.generate(text[0]['summary_text'])
@app.route('/propaganda')
def propaganda():
    url = request.args['url']
    goose = Goose()
    articles = goose.extract(url)
    output = queryprop({
	"inputs":  articles.cleaned_text[0:600]
    })
    
    yes = output[0][0]['score']
    no = 1 - yes
    return jsonify({"yes": yes, "no": no})

@app.route("/instagram")


# @app.route('/cloud')
# def plotly_wordcloud():
#     url = request.args['url']
#     goose = Goose()
#     articles = goose.extract(url)
#     text = query({
# 	"inputs":  articles.cleaned_text
#     })
#     wc = WordCloud(stopwords = set(STOPWORDS),
#                    max_words = 200,
#                    max_font_size = 100)
#     wc.generate(text[0]['summary_text'])
    
#     word_list=[]
#     freq_list=[]
#     fontsize_list=[]
#     position_list=[]
#     orientation_list=[]
#     color_list=[]

#     for (word, freq), fontsize, position, orientation, color in wc.layout_:
#         word_list.append(word)
#         freq_list.append(freq)
#         fontsize_list.append(fontsize)
#         position_list.append(position)
#         orientation_list.append(orientation)
#         color_list.append(color)
        
#     # get the positions
#     x=[]
#     y=[]
#     for i in position_list:
#         x.append(i[0])
#         y.append(i[1])
            
#     # get the relative occurence frequencies
#     new_freq_list = []
#     for i in freq_list:
#         new_freq_list.append(i*100)
#     new_freq_list
    
#     trace = go.Scatter(x=x, 
#                        y=y, 
#                        textfont = dict(size=new_freq_list,
#                                        color=color_list),
#                        hoverinfo='text',
#                        hovertext=['{0}{1}'.format(w, f) for w, f in zip(word_list, freq_list)],
#                        mode='text',  
#                        text=word_list
#                       )
    
#     layout = go.Layout({'xaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False},
#                         'yaxis': {'showgrid': False, 'showticklabels': False, 'zeroline': False}})
    
#     fig = go.Figure(data=[trace], layout=layout)
#     graphJSON = json.dumps(fig, cls=plotly.utils.PlotlyJSONEncoder)
#     print(graphJSON)
#     print(type(fig))
#     return graphJSON

@app.route('/authenticity')
def auth():
    url = request.args['url']
    lis = []
    df = pd.read_csv('blacklist.csv')
    for i in range(len(df)):
        lis.append(df.loc[i, "MBFC"])

    for l in lis:
        if(url.__contains__(l)):
            return {"authentic":False}

    return { "authentic": True }

@app.route('/bot-activity')
def botActivity():
    url = request.args['url']
    i=0
    usernames = []
    time = []
    finalusername = []
    for tweet in snstwitter.TwitterSearchScraper(url).get_items():
        usernames.append(tweet.user.username)
        time.append(tweet.date)
        if(i==150):
            break
        i+=1

    flag = False
    for i in range(len(time)-1):
        a = time[i]
        b = time[i+1]
        c = a-b
        if(c.seconds <= 60):            
            finalusername.append(usernames[i+1])

    print("username: ", finalusername)
    if(len(finalusername) > 3):
        flag = True
    return jsonify({"bots":list(set(finalusername)),"flag":flag})
#baseline model

    
from routes.news.articlesentiment import get_article_sentiment
app.register_blueprint(get_article_sentiment)

app.config.from_object(Config)

if __name__ == '__main__':
    app.run(debug=True)


import requests

@app.route('/gettweets')
def tweets():
    url = "https://cdn.syndication.twimg.com/tweet-result"

    querystring = {"id":"1652193613223436289","lang":"en"}

    payload = ""
    headers = {
        "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/114.0",
        "Accept": "*/*",
        "Accept-Language": "en-US,en;q=0.5",
        "Accept-Encoding": "gzip, deflate, br",
        "Origin": "https://platform.twitter.com",
        "Connection": "keep-alive",
        "Referer": "https://platform.twitter.com/",
        "Sec-Fetch-Dest": "empty",
        "Sec-Fetch-Mode": "cors",
        "Sec-Fetch-Site": "cross-site",
        "Pragma": "no-cache",
        "Cache-Control": "no-cache",
        "TE": "trailers"
    }

    response = requests.request("GET", url, data=payload, headers=headers, params=querystring)

    print(response.text)
    
